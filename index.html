<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-XE4HTS2YMF"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-XE4HTS2YMF');
    </script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation</title>

    <meta property='og:title' content='Perspective-Aware Reasoning. arXiv2025'/>
    <meta property='og:url' content='https://apc-vlm.github.io/'/>
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="stylesheet" href="./static/css/tab_gallery.css">
    <link rel="stylesheet" href="./static/css/image_card_fader.css">
    <link rel="stylesheet" href="./static/css/image_card_slider.css">
    <link rel="icon" href="./static/images/apc.ico">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="./static/js/magnifier.js"></script>
    <script type="text/javascript" async
        src="https://polyfill.io/v3/polyfill.min.js?features=es6">
    </script>
    <script type="text/javascript" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>
<body>
    <section class="hero banner">
        <div class="hero-body">
            <div class="container is-fluid">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <img src="./static/images/logo_apc.png" width="120px" style="margin-bottom: 20px;">
                        <h1 class="title is-2 publication-title">Perspective-Aware Reasoning in Vision-Language Models <br> via Mental Imagery Simulation</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://phillipinseoul.github.io/">Phillip Y. Lee<sup>1</sup></a>
                            </span>
                            <span class="author-block">
                                <a href="https://jihyeonje.com/">Jihyeon Je<sup>2</sup></a>
                            </span>
                            <span class="author-block">
                                <a href="https://charlieppark.kr/">Chanho Park<sup>1</sup></a>
                            </span>
                            <span class="author-block">
                                <a href="https://mikacuy.github.io/">Mikaela Angelina Uy<sup>3</sup></a>
                            </span>
                        </div>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://geometry.stanford.edu/?member=guibas">Leonidas Guibas<sup>2</sup></a>
                            </span>
                            <span class="author-block">
                                <a href="https://mhsung.github.io/">Minhyuk Sung<sup>1</sup></a>
                            </span>
                        </div>
                        <div class="publication-institution is-size-5">KAIST<sup>1</sup> &nbsp; Stanford University<sup>2</sup> &nbsp; NVIDIA<sup>3</sup></div>
                      
                        <br>
                        <div class="publication-links">
                            <span class="link-block">
                                <a href="" class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="fas fa-file-pdf"></i>
                                    </span>
                                    <span>Paper</span>
                                </a>
                            </span>
                            <span class="link-block">
                                <a href="" class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="ai ai-arxiv"></i>
                                    </span>
                                    <span>arXiv</span>
                                </a>
                            </span>
                            <span class="link-block">
                                <a href="https://github.com/KAIST-Visual-AI-Group/APC-VLM" class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="fab fa-github"></i>
                                    </span>
                                    <span>Code</span>
                                </a>
                            </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop has-text-centered">
                <h3 class="title is-5">TL;DR: TO ADD!</h3> 
            </div>
            <br>
            <div class="content has-text-centered">
                <img width="70%" src="./static/images/teaser_apc.jpg" class="interpolation-image"/>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3 has-text-centered">Abstract</h2>
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <div class="content has-text-justified">
                        <p>
                            We present a framework for perspective-aware reasoning in vision-language models (VLMs) through mental imagery simulation. 
                            Perspective-taking&mdash;the ability to perceive an environment or situation from an alternative viewpoint&mdash;is a key benchmark for 
                            human-level visual understanding, essential for environmental interaction and collaboration with autonomous agents. Despite 
                            advancements in spatial reasoning within VLMs, recent research has shown that modern VLMs significantly lack perspective-aware
                            reasoning capabilities and exhibit a strong bias toward egocentric interpretations. To bridge the gap between VLMs and human perception, 
                            we focus on the role of mental imagery, where humans perceive the world through abstracted representations that facilitate perspective 
                            shifts. Motivated by this, we propose a framework for perspective-aware reasoning, named Abstract Perspective Change (APC), 
                            that effectively leverages vision foundation models, such as object detection, segmentation, and orientation estimation, 
                            to construct scene abstractions and enable perspective transformations. Our experiments on synthetic and real-image benchmarks, 
                            compared with various VLMs, demonstrate significant improvements in perspective-aware reasoning with our framework, further outperforming 
                            fine-tuned spatial reasoning models and novel-view-synthesis-based approaches.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
        
    <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3 has-text-centered">üëÄ Problem Definition: Perspective-Aware Reasoning</h2>
            <div class="content has-text-centered">
                <img src="./static/images/ego_and_allo.jpg" width="100%">
            </div>
            <div class="content has-text-justified">
                While VLMs perform well when questions are asked from the egocentric (i.e. camera's) perspective, they struggle when the same questions are posed from an 
                allocentric perspective, showing a strong bias toward egocentric reasoning [<a href="#ref1">1</a>]. Allocentric reasoning is crucial for high-level spatial tasks, and 
                serves as a key benchmark for human-level spatial understanding, as also recognized in previous studies on VLM spatial reasoning [<a href="#ref1">1</a>, <a href="#ref2">2</a>, <a href="#ref3">3</a>, <a href="#ref4">4</a>, <a href="#ref5">5</a>].
                <b>In this work, we aim to extend the spatial reasoning capabilities of VLMs to üåüarbitrary perspectivesüåü</b>, thereby bridging the gap between VLMs and human perception and opening 
                up new possibilities for VLM-based applications.
            </div>
            <br>
            <h2 class="title is-3 has-text-centered">üí° Main Idea: Abstract Perspective Change</h2>
            <h3 class="title is-4">TODO.</h3>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3 has-text-centered">üñºÔ∏è Results</h2>
            <div class="content has-text-justified">
                <p>
                    TODO.
                </p>
            </div>
    </section>


    <!-- <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3 has-text-centered">Acknowledgement</h2>
            <div class="content has-text-justified">
                <p>
                    We thank <a href="https://dvelopery0115.github.io/"> Seungwoo Yoo</a> and <a href="https://63days.github.io/"> Juil Koo</a> for providing constructive feedback of our manuscript. 
                    Thank you to <a href="https://phillipinseoul.github.io/"> Phillip Y. Lee</a> for helpful discussions on Vision Language Models. 
                </p>
            </div>
        </div>
    </div>
    </section> -->

    <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3 has-text-centered">References</h2>
            <span id="ref1">[1]</span> <a href="https://arxiv.org/abs/2410.17385">Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities, Zhang et al., ICLR 2025</a> <br>
            <span id="ref2">[2]</span> <a href="https://arxiv.org/abs/2412.07825">3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark, Ma et al., arXiv 2024</a> <br>
            <span id="ref3">[3]</span> <a href="https://arxiv.org/abs/2409.12969">Seeing Through Their Eyes: Evaluating Visual Perspective Taking in Vision Language Models, G√≥ral et al., arXiv 2024</a> <br>
            <span id="ref4">[4]</span> <a href="https://arxiv.org/abs/2406.04138">The 3D-PC: A Benchmark for Visual Perspective Taking in Humans and Machines, Linsley et al., ICLR 2025</a> <br>
            <span id="ref5">[5]</span> <a href="https://arxiv.org/abs/2412.14171">Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces, Yang et al., CVPR 2025</a> <br>
          </div>
        </div>
    </section>

    <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title"><a id="bibtex">BibTeX</a></h2>
        <pre style="white-space: pre-wrap;"><code>@article{kim2025inference,
        title  = {Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing},
        author={Kim, Jaihoon and Yoon, Taehoon and Hwang, Jisung and Sung, Minhyuk},
        journal={arXiv preprint arXiv:2503.19385},
        year   = {2025}}</code></pre>
    </div> -->

    

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content" style="text-align: left;">
          <h2 class="title is-4"><a id="bibtex">Citation</a></h2>
          <p>If you find our work helpful, please cite the following paper.</p>
      
          <div style="position: relative; background-color: #ffffff; padding: 0.5em 0.75em; border-radius: 4px; font-family: monospace; color: #000; overflow-x: auto; border: 1px solid #ddd; display: inline-block; text-align: left;">
            <button 
              id="copy-button"
              onclick="copyBibtex()" 
              style="position: absolute; top: 6px; right: 6px; font-size: 0.75rem; background: #f5f5f5; color: #333; border: 1px solid #ccc; border-radius: 3px; padding: 0.2em 0.4em; cursor: pointer;">
              üìã
            </button>
            <span id="copied-msg" style="display: none; position: absolute; top: 6px; right: 6px; font-size: 0.75rem; background: #e8ffe8; color: #27ae60; border: 1px solid #27ae60; border-radius: 3px; padding: 0.2em 0.4em;">
              ‚úÖ
            </span>
            <code id="bibtex-code" style="white-space: pre; background: none;">
      @article{kim2025inference,
        title  = {Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing},
        author = {Kim, Jaihoon and Yoon, Taehoon and Hwang, Jisung and Sung, Minhyuk},
        journal= {arXiv preprint arXiv:2503.19385},
        year   = {2025}
      }
            </code>
          </div>
      
          <script>
            function copyBibtex() {
              const code = document.getElementById("bibtex-code").innerText;
              navigator.clipboard.writeText(code).then(() => {
                document.getElementById("copy-button").style.display = "none";
                document.getElementById("copied-msg").style.display = "inline-block";
                setTimeout(() => {
                  document.getElementById("copied-msg").style.display = "none";
                  document.getElementById("copy-button").style.display = "inline-block";
                }, 1500);
              });
            }
          </script>
        </div>
      </section>
      
      
      
      



    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>
                        <p>
                            Website adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, <a href="https://joonghyuk.com/instantdrag-web/">InstantDrag</a> and <a href="https://flow-inference-time-scaling.github.io/">RBF</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>
</body>
</html>