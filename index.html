<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-XE4HTS2YMF"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-XE4HTS2YMF');
    </script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation</title>

    <meta property='og:title' content='Perspective-Aware Reasoning. arXiv2025'/>
    <meta property='og:url' content='https://apc-vlm.github.io/'/>
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="stylesheet" href="./static/css/tab_gallery.css">
    <link rel="stylesheet" href="./static/css/image_card_fader.css">
    <link rel="stylesheet" href="./static/css/image_card_slider.css">
    <link rel="icon" href="./static/images/apc.ico">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="./static/js/magnifier.js"></script>
    <script type="text/javascript" async
        src="https://polyfill.io/v3/polyfill.min.js?features=es6">
    </script>
    <script type="text/javascript" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>
<body>
    <section class="hero banner">
        <div class="hero-body">
            <div class="container is-fluid">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <img src="./static/images/logo_apc.png" width="120px" style="margin-bottom: 20px;">
                        <h1 class="title is-2 publication-title">Perspective-Aware Reasoning in Vision-Language Models <br> via Mental Imagery Simulation</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://phillipinseoul.github.io/">Phillip Y. Lee<sup>1</sup></a>
                            </span>
                            <span class="author-block">
                                <a href="https://jihyeonje.com/">Jihyeon Je<sup>2</sup></a>
                            </span>
                            <span class="author-block">
                                <a href="https://charlieppark.kr/">Chanho Park<sup>1</sup></a>
                            </span>
                            <span class="author-block">
                                <a href="https://mikacuy.github.io/">Mikaela Angelina Uy<sup>3</sup></a>
                            </span>
                        </div>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://geometry.stanford.edu/?member=guibas">Leonidas Guibas<sup>2</sup></a>
                            </span>
                            <span class="author-block">
                                <a href="https://mhsung.github.io/">Minhyuk Sung<sup>1</sup></a>
                            </span>
                        </div>
                        <div class="publication-institution is-size-5">KAIST<sup>1</sup> &nbsp; Stanford University<sup>2</sup> &nbsp; NVIDIA<sup>3</sup></div>
                      
                        <br>
                        <div class="publication-links">
                            <span class="link-block">
                                <a href="./static/Perspective_Aware_Reasoning_Preprint.pdf" class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="fas fa-file-pdf"></i>
                                    </span>
                                    <span>Paper</span>
                                </a>
                            </span>
                            <span class="link-block">
                                <a href="" class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="ai ai-arxiv"></i>
                                    </span>
                                    <span>arXiv</span>
                                </a>
                            </span>
                            <span class="link-block">
                                <a href="https://github.com/KAIST-Visual-AI-Group/APC-VLM" class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="fab fa-github"></i>
                                    </span>
                                    <span>Code</span>
                                </a>
                            </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop has-text-centered">
                <h3 class="title is-5"">
                    üìù TL;DR: With our framework, VLMs can perform spatial reasoning in arbitrary perspectives.
                </h3> 
            </div>
            <br>
            <div class="content has-text-centered">
                <img width="70%" src="./static/images/teaser_apc.jpg" class="interpolation-image"/>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3 has-text-centered">Abstract</h2>
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <div class="content has-text-justified">
                        <p>
                            We present a framework for perspective-aware reasoning in vision-language models (VLMs) through mental imagery simulation. 
                            Perspective-taking&mdash;the ability to perceive an environment or situation from an alternative viewpoint&mdash;is a key benchmark for 
                            human-level visual understanding, essential for environmental interaction and collaboration with autonomous agents. Despite 
                            advancements in spatial reasoning within VLMs, recent research has shown that modern VLMs significantly lack perspective-aware
                            reasoning capabilities and exhibit a strong bias toward egocentric interpretations. To bridge the gap between VLMs and human perception, 
                            we focus on the role of mental imagery, where humans perceive the world through abstracted representations that facilitate perspective 
                            shifts. Motivated by this, we propose a framework for perspective-aware reasoning, named Abstract Perspective Change (APC), 
                            that effectively leverages vision foundation models, such as object detection, segmentation, and orientation estimation, 
                            to construct scene abstractions and enable perspective transformations. Our experiments on synthetic and real-image benchmarks, 
                            compared with various VLMs, demonstrate significant improvements in perspective-aware reasoning with our framework, further outperforming 
                            fine-tuned spatial reasoning models and novel-view-synthesis-based approaches.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
        
    <section class="section">
        <div class="container is-max-desktop">
            <hr>
            <h2 class="title is-3 has-text-centered">üëÄ Problem Definition</h2>
            <h3 class="title is-6 has-text-centered">Perspectives are important, but remain a missing block in VLM spatial reasoning.</h3>
            <div class="content has-text-centered">
                <img src="./static/images/ego_and_allo.jpg" width="100%">
            </div>
            <div class="content has-text-justified">
                <b>Egocentric vs. Allocentric.</b>
                While VLMs perform well when questions are asked from the egocentric (i.e. camera's) perspective, they struggle when the same questions are posed from an 
                allocentric perspective, showing a strong bias toward egocentric reasoning [<a href="#ref1">1</a>]. Allocentric reasoning is crucial for high-level spatial tasks, and 
                serves as a key benchmark for human-level spatial understanding, as also recognized in previous studies on VLM spatial reasoning [<a href="#ref1">1</a>, <a href="#ref2">2</a>, <a href="#ref3">3</a>, <a href="#ref4">4</a>, <a href="#ref5">5</a>].
                <b>In this work, we aim to extend the spatial reasoning capabilities of VLMs to üåüarbitrary perspectivesüåü</b>, thereby bridging the gap between VLMs and human perception and opening 
                up new possibilities for VLM-based applications.
            </div>

            <br><hr>
            <h2 class="title is-3 has-text-centered">ü§î How Do <i>Humans</i> Shift Perspectives?</h2>
            <h3 class="title is-6 has-text-centered">We can imagine a scene from another viewpoint by forming an <i>abstract</i> version of it and rotating it in our mind.</h3>
            <div class="content has-text-centered">
                <img src="./static/images/mental_imagery_concept.jpg" width="50%">
            </div>
            <div class="content has-text-justified">
                <b>Mental Imagery Simulation.</b>
                Inspired by how humans employ mental imagery [<a href="#ref6">6</a>, <a href="#ref7">7</a>, <a href="#ref8">8</a>, <a href="#ref9">9</a>] to reason from across different perspectives (left), we propose a similar process for VLMs, 
                by constructing an explicit abstraction of the input scene and using it as a foundation for perspective changes (right).
            </div>
            <div class="content has-text-justified">
                Based on this idea, we build our framework by answering the following questions:
                <ol>
                    <b><li>Given an image of a scene and a question based on it, how can we turn it into an abstract 3D scene?</li></b>
                    <b><li>After transforming the abstract scene to align with a given perspective, how can we feed it back to the VLM?</li></b>
                </ol>
            </div>
            <br><hr>
            <h2 class="title is-3 has-text-centered">üí° Abstract Perspective Change</h2>
            <h3 class="title is-6 has-text-centered">
                We propose <i>APC</i>, a framework that enables VLMs to perform perspective-aware reasoning, <br>by explicitly simulating the mental imagery process of humans.
            </h3>
            <div class="content has-text-centered">
                <img src="./static/images/main_pipeline_apc.jpg" width="100%">
            </div>
            <div class="content has-text-justified">
                Our proposed framework consists of three stages:
                <ol>
                    <li><b>Scene Abstraction (Sec. 3.1)</b>: APC first detects the objects of interest and build a coarse 3D abstraction of the scene using off-the-shelf vision foundation models.
                    We employ models for object detection [<a href="#ref12">12</a>], segmentation [<a href="#ref13">13</a>], depth estimation [<a href="#ref14">14</a>], and orientation estimation [<a href="#ref15">15</a>].</li>
                    <li><b>Perspective Change (Sec. 3.2)</b>: Then, a reference perspective is set and the abstraction is transformed into the reference viewer‚Äôs egocentric coordinate frame.</li>
                    <li><b>Perspective Prompting (Sec. 3.3)</b>: Finally, APC passes the transformed scene to the VLM by producing (1) a numerical (textual) prompt or (2) an abstract visual prompt, and poses the question of interest from the reference perspective.</li>
                </ol>
            </div>
            <br>
            <h2 class="title is-3 has-text-centered">üîç Perspective Prompts</h2>
            <h3 class="title is-6 has-text-centered">
                In APC, the transformed abstract scene is then passed to the VLM in the form of a <i>prompt</i>.
            </h3>
            <div class="content has-text-centered">
                <img src="./static/images/perspective_prompts_apc.jpg" width="100%">
            </div>
            <div class="content has-text-justified">
                <b>Perspective Prompt Samples.</b> We explore two variations of perspective prompting, <b>numerical (left)</b> and <b>visual (right)</b>. 
                Numerical (textual) prompt is generated by directly utilizing the 3D coordinate and orientation information. 
                To generate the Visual prompt, we first place a colored cube at each object‚Äôs identified 3D position then render the scene at the reference viewpoint, which results in an egocentric depiction of the scene.
                In addition, we construct an abstract question along with object-color mapping to ground the abstracted view.
            </div>
        </div>
    </section>
    <section class="section">
        <div class="container is-max-desktop">
            <hr>
            <h2 class="title is-3 has-text-centered">üñºÔ∏è Results</h2>
            <h3 class="title is-6 has-text-centered">
                Comparison with Baseline VLMs.
            </h3>
            <div class="content has-text-centered">
                <img src="./static/images/qualitatives.png" width="100%">
            </div>
            <div class="content has-text-justified">
                <p>
                    <b>Spatial Reasoning with Perspective Change.</b>
                    Recent VLMs such as Qwen2.5-VL [<a href="#ref10">10</a>] and Cambrian-1 [<a href="#ref11">11</a>] often struggle with spatial reasoning tasks that require 
                    a shift to a specific reference viewpoint. In constrast, our APC effectively handles such perspective changes by constructing a scene abstraction 
                    and delivering the transformed view through a simple prompting technique.
                </p>
            </div>
            <br>
            <h3 class="title is-6 has-text-centered">
                Probing the Perspective Awareness of VLMs.
            </h3>
            <div class="content has-text-centered">
                <img src="./static/images/perspective_awareness.png" width="60%">
            </div>
            <div class="content has-text-justified">
                <p>
                    <b>Perspective Awareness.</b>
                    Each plot shows accuracy versus the angular offset Œ∏ between the camera and the reference viewpoint. 
                    While baselines show clear degradation at certain ranges of Œ∏, APC retains robust accuracy across all angles, demonstrating strong perspective-aware reasoning.
                </p>
            </div>
    </section>


    <!-- <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3 has-text-centered">Acknowledgement</h2>
            <div class="content has-text-justified">
                <p>
                    We thank <a href="https://dvelopery0115.github.io/"> Seungwoo Yoo</a> and <a href="https://63days.github.io/"> Juil Koo</a> for providing constructive feedback of our manuscript. 
                    Thank you to <a href="https://phillipinseoul.github.io/"> Phillip Y. Lee</a> for helpful discussions on Vision Language Models. 
                </p>
            </div>
        </div>
    </div>
    </section> -->

    <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3 has-text-centered">References</h2>
            <span id="ref1">[1]</span> <a href="https://arxiv.org/abs/2410.17385">Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities, Zhang et al., ICLR 2025</a> <br>
            <span id="ref2">[2]</span> <a href="https://arxiv.org/abs/2412.07825">3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark, Ma et al., arXiv 2024</a> <br>
            <span id="ref3">[3]</span> <a href="https://arxiv.org/abs/2409.12969">Seeing Through Their Eyes: Evaluating Visual Perspective Taking in Vision Language Models, G√≥ral et al., arXiv 2024</a> <br>
            <span id="ref4">[4]</span> <a href="https://arxiv.org/abs/2406.04138">The 3D-PC: A Benchmark for Visual Perspective Taking in Humans and Machines, Linsley et al., ICLR 2025</a> <br>
            <span id="ref5">[5]</span> <a href="https://arxiv.org/abs/2412.14171">Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces, Yang et al., CVPR 2025</a> <br>
            <span id="ref6">[6]</span> <a href="https://psycnet.apa.org/record/1989-98796-000">Principles of mental imagery., Finke, R. A, The MIT Press, 1989</a> <br>
            <span id="ref7">[7]</span> <a href="https://plato.stanford.edu/entries/mental-imagery/">Mental imagery., Bence Nanay, The Stanford Encyclopedia of Philosophy, 1997</a> <br>
            <span id="ref8">[8]</span> <a href="https://pubmed.ncbi.nlm.nih.gov/5540314/">Mental rotation of three-dimensional objects., Roger N Shepard and Jacqueline Metzler, Science, 1971</a> <br>
            <span id="ref9">[9]</span> <a href="https://psycnet.apa.org/record/1979-00241-001">Visual images preserve metric spatial information: Evidence from studies of image scanning., Kosslyn et al., Journal of Experimental Psychology, 1978</a> <br>
            <span id="ref10">[10]</span> <a href="https://arxiv.org/abs/2502.13923">Qwen2.5-VL Technical Report, Qwen Team Alibaba Group, arXiv 2025</a> <br>
            <span id="ref11">[11]</span> <a href="https://arxiv.org/abs/2406.16860">Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs, NeurIPS 2024</a> <br>
            <span id="ref12">[12]</span> <a href="https://arxiv.org/abs/2303.05499">Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection, Liu et al., ECCV 2024</a> <br>
            <span id="ref13">[13]</span> <a href="https://arxiv.org/abs/2404.01683">Segment Anything, Kirillov et al., ICCV 2023</a> <br>
            <span id="ref14">[14]</span> <a href="https://arxiv.org/abs/2410.02073">Depth Pro: Sharp Monocular Metric Depth in Less Than a Second, Bochkovskii et al., ICLR 2025</a> <br>
            <span id="ref15">[15]</span> <a href="https://arxiv.org/abs/2404.01683">Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models, Wang et al., arXiv 2024</a> <br>
          </div>
        </div>
    </section>

    <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title"><a id="bibtex">BibTeX</a></h2>
        <pre style="white-space: pre-wrap;"><code>@article{kim2025inference,
        title  = {Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing},
        author={Kim, Jaihoon and Yoon, Taehoon and Hwang, Jisung and Sung, Minhyuk},
        journal={arXiv preprint arXiv:2503.19385},
        year   = {2025}}</code></pre>
    </div> -->

    

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content" style="text-align: left;">
          <h2 class="title is-4"><a id="bibtex">Citation</a></h2>
          <p>If you find our work helpful, please cite the following paper.</p>
      
          <div style="position: relative; background-color: #ffffff; padding: 0.5em 0.75em; border-radius: 4px; font-family: monospace; color: #000; overflow-x: auto; border: 1px solid #ddd; display: inline-block; text-align: left;">
            <button 
              id="copy-button"
              onclick="copyBibtex()" 
              style="position: absolute; top: 6px; right: 6px; font-size: 0.75rem; background: #f5f5f5; color: #333; border: 1px solid #ccc; border-radius: 3px; padding: 0.2em 0.4em; cursor: pointer;">
              üìã
            </button>
            <span id="copied-msg" style="display: none; position: absolute; top: 6px; right: 6px; font-size: 0.75rem; background: #e8ffe8; color: #27ae60; border: 1px solid #27ae60; border-radius: 3px; padding: 0.2em 0.4em;">
              ‚úÖ
            </span>
            <code id="bibtex-code" style="white-space: pre; background: none;">
      @article{lee2025perspective,
        title  = {Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation},
        author = {Lee, Phillip Y. and Je, Jihyeon and Park, Chanho and Uy, Mikaela Angelina and Guibas, Leonidas and Sung, Minhyuk},
        journal= {arXiv preprint arXiv:2504.17207},
        year   = {2025}
      }
            </code>
          </div>
      
          <script>
            function copyBibtex() {
              const code = document.getElementById("bibtex-code").innerText;
              navigator.clipboard.writeText(code).then(() => {
                document.getElementById("copy-button").style.display = "none";
                document.getElementById("copied-msg").style.display = "inline-block";
                setTimeout(() => {
                  document.getElementById("copied-msg").style.display = "none";
                  document.getElementById("copy-button").style.display = "inline-block";
                }, 1500);
              });
            }
          </script>
        </div>
      </section>
      
      
      
      



    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>
                        <p>
                            Website adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, <a href="https://joonghyuk.com/instantdrag-web/">InstantDrag</a> and <a href="https://flow-inference-time-scaling.github.io/">RBF</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>
</body>
</html>